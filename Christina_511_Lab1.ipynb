{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer, MWETokenizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "import gensim\n",
        "from gensim.utils import tokenize\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPWIdrvMT2Iu",
        "outputId": "d148f0c5-111e-4339-d47a-fbcf81b140e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paragraph"
      ],
      "metadata": {
        "id": "C8kZ-LnXT_ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"The sun ‚òÄÔ∏è rose gracefully over the horizon, painting the sky in hues of pink and orange üåÖ. Birds üê¶ chirped cheerfully as they welcomed the new day, while a gentle breeze üçÉ whispered through the trees üå≥. In the distance, the sound of waves üåä crashing against the shore added a soothing rhythm to the morning symphony. It was a perfect moment, filled with peace ‚úåÔ∏è and tranquility.\""
      ],
      "metadata": {
        "id": "AosLkVJFVUT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Tokenisation\n",
        "\n",
        "\n",
        "Word tokenization refers to the process of breaking down a text or a sentence into individual words or tokens. These tokens are the basic units of meaning in a language and are often used as input for various NLP tasks such as text classification, sentiment analysis, and machine translation."
      ],
      "metadata": {
        "id": "Xev4RCAhWVE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(para)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZFtFTg0Wi2w",
        "outputId": "64cba3c8-117d-4ce1-90fb-cc99d4210261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'sun', '‚òÄÔ∏è', 'rose', 'gracefully', 'over', 'the', 'horizon', ',', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'pink', 'and', 'orange', 'üåÖ', '.', 'Birds', 'üê¶', 'chirped', 'cheerfully', 'as', 'they', 'welcomed', 'the', 'new', 'day', ',', 'while', 'a', 'gentle', 'breeze', 'üçÉ', 'whispered', 'through', 'the', 'trees', 'üå≥', '.', 'In', 'the', 'distance', ',', 'the', 'sound', 'of', 'waves', 'üåä', 'crashing', 'against', 'the', 'shore', 'added', 'a', 'soothing', 'rhythm', 'to', 'the', 'morning', 'symphony', '.', 'It', 'was', 'a', 'perfect', 'moment', ',', 'filled', 'with', 'peace', '‚úåÔ∏è', 'and', 'tranquility', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Tokenization\n",
        "\n",
        "Sentence tokenization involves splitting a text or a corpus into individual sentences. Used for text summarization, sentiment analysis, and machine translation, where understanding the meaning and structure of individual sentences is essential for accurate analysis and interpretation.\n",
        "\n"
      ],
      "metadata": {
        "id": "AJMY9M_tW1GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = sent_tokenize(para)\n",
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_sY-8EJXMqB",
        "outputId": "e091a52b-f2ff-4871-9603-18a4d9939608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The sun ‚òÄÔ∏è rose gracefully over the horizon, painting the sky in hues of pink and orange üåÖ.', 'Birds üê¶ chirped cheerfully as they welcomed the new day, while a gentle breeze üçÉ whispered through the trees üå≥.', 'In the distance, the sound of waves üåä crashing against the shore added a soothing rhythm to the morning symphony.', 'It was a perfect moment, filled with peace ‚úåÔ∏è and tranquility.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Punctuation-based Tokenizer\n",
        "\n",
        "\n",
        "In Natural Language Processing (NLP), a Punctuation-based Tokenizer is a type of tokenizer specifically designed to break down text into tokens based on punctuation marks. This tokenizer splits text into tokens wherever it encounters punctuation characters such as periods, commas, question marks, exclamation marks, etc."
      ],
      "metadata": {
        "id": "WIdF6b9XXc8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punkt_tokenizer = WordPunctTokenizer()\n",
        "punkt_tokens = punkt_tokenizer.tokenize(para)\n",
        "print(punkt_tokenizer)\n",
        "print(punkt_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHOBPRBWX2Hf",
        "outputId": "a7eeaeee-4678-4f0c-da5a-438cb75752fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL)\n",
            "['The', 'sun', '‚òÄÔ∏è', 'rose', 'gracefully', 'over', 'the', 'horizon', ',', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'pink', 'and', 'orange', 'üåÖ.', 'Birds', 'üê¶', 'chirped', 'cheerfully', 'as', 'they', 'welcomed', 'the', 'new', 'day', ',', 'while', 'a', 'gentle', 'breeze', 'üçÉ', 'whispered', 'through', 'the', 'trees', 'üå≥.', 'In', 'the', 'distance', ',', 'the', 'sound', 'of', 'waves', 'üåä', 'crashing', 'against', 'the', 'shore', 'added', 'a', 'soothing', 'rhythm', 'to', 'the', 'morning', 'symphony', '.', 'It', 'was', 'a', 'perfect', 'moment', ',', 'filled', 'with', 'peace', '‚úåÔ∏è', 'and', 'tranquility', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tweet Tokenizer\n",
        "The Tweet Tokenizer is  for tokenizing tweets or short social media messages. Unlike traditional tokenizers which may split text based on spaces or punctuation alone, the Tweet Tokenizer takes into account the unique characteristics of tweets, such as hashtags, mentions, URLs, and emojis."
      ],
      "metadata": {
        "id": "VE4-8SnqX2zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(para)\n",
        "print(tweet_tokenizer)\n",
        "print(tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSr2JZ_GYQYi",
        "outputId": "6c0c8d31-b349-4213-9c3f-40a8d2a265a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<nltk.tokenize.casual.TweetTokenizer object at 0x7b5ee75ac4c0>\n",
            "['The', 'sun', '‚òÄ', 'Ô∏è', 'rose', 'gracefully', 'over', 'the', 'horizon', ',', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'pink', 'and', 'orange', 'üåÖ', '.', 'Birds', 'üê¶', 'chirped', 'cheerfully', 'as', 'they', 'welcomed', 'the', 'new', 'day', ',', 'while', 'a', 'gentle', 'breeze', 'üçÉ', 'whispered', 'through', 'the', 'trees', 'üå≥', '.', 'In', 'the', 'distance', ',', 'the', 'sound', 'of', 'waves', 'üåä', 'crashing', 'against', 'the', 'shore', 'added', 'a', 'soothing', 'rhythm', 'to', 'the', 'morning', 'symphony', '.', 'It', 'was', 'a', 'perfect', 'moment', ',', 'filled', 'with', 'peace', '‚úå', 'Ô∏è', 'and', 'tranquility', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-Word Expression Tokenizer\n",
        "\n",
        "Multi-Word Expression Tokenizer is a tool or algorithm designed to identify and tokenize multi-word expressions (MWEs) within a text. MWEs are sequences of words that often appear together and convey a specific meaning that may not be predictable from the individual words alone."
      ],
      "metadata": {
        "id": "36v0dLMVYSbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Word Expression Tokenizer\n",
        "multi_word_expr = [(\"and\", \"sun\"), (\"processing\", \"NLP\")]\n",
        "mwe_tokenizer = MWETokenizer(multi_word_expr)\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(para))\n",
        "print(mwe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F93IS43sYYV2",
        "outputId": "93988dad-6f17-4fa2-eba3-5dc81d95c167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'sun', '‚òÄÔ∏è', 'rose', 'gracefully', 'over', 'the', 'horizon', ',', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'pink', 'and', 'orange', 'üåÖ', '.', 'Birds', 'üê¶', 'chirped', 'cheerfully', 'as', 'they', 'welcomed', 'the', 'new', 'day', ',', 'while', 'a', 'gentle', 'breeze', 'üçÉ', 'whispered', 'through', 'the', 'trees', 'üå≥', '.', 'In', 'the', 'distance', ',', 'the', 'sound', 'of', 'waves', 'üåä', 'crashing', 'against', 'the', 'shore', 'added', 'a', 'soothing', 'rhythm', 'to', 'the', 'morning', 'symphony', '.', 'It', 'was', 'a', 'perfect', 'moment', ',', 'filled', 'with', 'peace', '‚úåÔ∏è', 'and', 'tranquility', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TextBlob Word Tokenize\n",
        "\n",
        "\n",
        "TextBlob refers to the process of breaking down a text into individual words or tokens. TextBlob provides a method called word_tokenize() which takes a string of text as input and returns a list of words or tokens extracted from that text. This function splits the text based on whitespace and punctuation, effectively separating it into its constituent words."
      ],
      "metadata": {
        "id": "knusoK22YkTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textblob_tokens = TextBlob(para).words\n",
        "print(textblob_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBz_7IoqYeAA",
        "outputId": "c89b249d-026f-49ac-ab9b-cb4386afa958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'sun', '‚òÄÔ∏è', 'rose', 'gracefully', 'over', 'the', 'horizon', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'pink', 'and', 'orange', 'üåÖ', 'Birds', 'üê¶', 'chirped', 'cheerfully', 'as', 'they', 'welcomed', 'the', 'new', 'day', 'while', 'a', 'gentle', 'breeze', 'üçÉ', 'whispered', 'through', 'the', 'trees', 'üå≥', 'In', 'the', 'distance', 'the', 'sound', 'of', 'waves', 'üåä', 'crashing', 'against', 'the', 'shore', 'added', 'a', 'soothing', 'rhythm', 'to', 'the', 'morning', 'symphony', 'It', 'was', 'a', 'perfect', 'moment', 'filled', 'with', 'peace', '‚úåÔ∏è', 'and', 'tranquility']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#spaCy tokenizer\n",
        "\n",
        "\n",
        "spaCy  is a component of the spaCy library that is responsible for breaking down a text into individual tokens, such as words or punctuation marks."
      ],
      "metadata": {
        "id": "JoZqd6RuYqA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "spacy_tokens = [token.text for token in nlp(para)]\n",
        "print(spacy_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INOb32fyYfh6",
        "outputId": "c2ca4299-2aed-4146-cccc-687a5eeabee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'sun', '‚òÄ', 'Ô∏è', 'rose', 'gracefully', 'over', 'the', 'horizon', ',', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'pink', 'and', 'orange', 'üåÖ', '.', 'Birds', 'üê¶', 'chirped', 'cheerfully', 'as', 'they', 'welcomed', 'the', 'new', 'day', ',', 'while', 'a', 'gentle', 'breeze', 'üçÉ', 'whispered', 'through', 'the', 'trees', 'üå≥', '.', 'In', 'the', 'distance', ',', 'the', 'sound', 'of', 'waves', 'üåä', 'crashing', 'against', 'the', 'shore', 'added', 'a', 'soothing', 'rhythm', 'to', 'the', 'morning', 'symphony', '.', 'It', 'was', 'a', 'perfect', 'moment', ',', 'filled', 'with', 'peace', '‚úå', 'Ô∏è', 'and', 'tranquility', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Genism Word tokenizer\n",
        "\n",
        "The gensim.utils.tokenize() function is a part of Gensim's utilities for text processing. It tokenizes the input text (para) into individual tokens or words. The output gensim_tokens would be an iterable object containing the tokens extracted from the paragraph."
      ],
      "metadata": {
        "id": "d79nAzbkYymp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gensim_tokens = gensim.utils.tokenize(para)\n",
        "gensim_tokens = list(tokenize(para))\n",
        "print(gensim_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRTfm-1OYhjd",
        "outputId": "4b91cbb1-9734-4f0b-9371-b0bf71e3ae07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'sun', 'rose', 'gracefully', 'over', 'the', 'horizon', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'pink', 'and', 'orange', 'Birds', 'chirped', 'cheerfully', 'as', 'they', 'welcomed', 'the', 'new', 'day', 'while', 'a', 'gentle', 'breeze', 'whispered', 'through', 'the', 'trees', 'In', 'the', 'distance', 'the', 'sound', 'of', 'waves', 'crashing', 'against', 'the', 'shore', 'added', 'a', 'soothing', 'rhythm', 'to', 'the', 'morning', 'symphony', 'It', 'was', 'a', 'perfect', 'moment', 'filled', 'with', 'peace', 'and', 'tranquility']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization with Keras in Python ML\n",
        "Keras is a high-level neural networks API written in Python, which can be used for various machine learning tasks, including NLP.  With Keras, tokenization can be easily implemented using its built-in text preprocessing utilities, such as the Tokenizer class. This class provides methods to tokenize text, convert it into sequences of integers (which represent the tokens), and handle aspects like padding and truncating sequences to ensure consistent input sizes for neural networks."
      ],
      "metadata": {
        "id": "1gVmTtOMY4vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization with Keras in Python ML\n",
        "keras_tokens = text_to_word_sequence(para)\n",
        "print(keras_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5ue-KiaYid2",
        "outputId": "6a3d6170-e49c-43c1-c04f-fd0eb25d0ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'sun', '‚òÄÔ∏è', 'rose', 'gracefully', 'over', 'the', 'horizon', 'painting', 'the', 'sky', 'in', 'hues', 'of', 'pink', 'and', 'orange', 'üåÖ', 'birds', 'üê¶', 'chirped', 'cheerfully', 'as', 'they', 'welcomed', 'the', 'new', 'day', 'while', 'a', 'gentle', 'breeze', 'üçÉ', 'whispered', 'through', 'the', 'trees', 'üå≥', 'in', 'the', 'distance', 'the', 'sound', 'of', 'waves', 'üåä', 'crashing', 'against', 'the', 'shore', 'added', 'a', 'soothing', 'rhythm', 'to', 'the', 'morning', 'symphony', 'it', 'was', 'a', 'perfect', 'moment', 'filled', 'with', 'peace', '‚úåÔ∏è', 'and', 'tranquility']\n"
          ]
        }
      ]
    }
  ]
}